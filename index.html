<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="PhysicEdit"/>
  <meta property="og:description" content="From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="diffusion, t2i, reflection, dit">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors</title>
  <link rel="icon" type="image/x-icon" href="static/images/papericon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">From Statics to Dynamics: Physics-Aware Image Editing with Latent Transition Priors</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <a href="https://liangbingzhao.github.io/" style="color:#008AD7;font-weight:normal;">Liangbing Zhao</a><sup style="color:#6fbf73;">1,</sup></span>
              <a href="https://le-zhuo.com/" style="color:#008AD7;font-weight:normal;">Le Zhuo</a><sup style="color:#ed4b82;">2</sup>,</span>
              <a href="https://sayak.dev/" style="color:#008AD7;font-weight:normal;">Sayak Paul</a><sup style="color:#ffac33;">3</sup>,</span>
              <a href="https://www.ee.cuhk.edu.hk/~hsli/" style="color:#008AD7;font-weight:normal;">Hongsheng Li</a><sup style="color:#ed4b82;">2</sup>,</span>
              <a href="https://cemse.kaust.edu.sa/profiles/mohamed-elhoseiny" style="color:#008AD7;font-weight:normal;">Mohamed Elhoseiny</a><sup style="color:#6fbf73;">1</sup></span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block"><sup style="color:#6fbf73;">1</sup>KAUST</span>&nbsp;
              <span class="author-block"><sup style="color:#ed4b82;">2</sup>CUHK MMLAB</span>
              <span class="author-block"><sup style="color:#ffac33;">3</sup>Hugging Face</span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/liangbingzhao/PhysicEdit" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- HF link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/collections/metazlb/physicedit-release" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="font-size:18px">ðŸ¤—</span>
                      <span>PhysicEdit</span>
                    </a>
                  </span>       
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Instruction-based image editing has achieved remarkable success in semantic alignment, yet state-of-the-art models frequently fail to render physically plausible results when editing involves complex causal dynamics, such as refraction or material deformation. We attribute this limitation to the dominant paradigm that treats editing as a discrete mapping between image pairs, which provides only boundary conditions and leaves transition dynamics underspecified.  To address this, we reformulate physics-aware editing as predictive physical state transitions and introduce PhysicTran38K, a large-scale video-based dataset comprising 38K transition trajectories across five physical domains, constructed via a two-stage filtering and constraint-aware annotation pipeline. Building on this supervision, we propose PhysicEdit, an end-to-end framework equipped with a textual-visual dual-thinking mechanism. It combines a frozen Qwen2.5-VL for physically grounded reasoning with learnable transition queries that provide timestep-adaptive visual guidance to a diffusion backbone. Experiments show that PhysicEdit improves over Qwen-Image-Edit by 5.9% in physical realism and 10.1% in knowledge-grounded editing, setting a new state-of-the-art for open-source methods, while remaining competitive with leading proprietary models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Motivation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Motivation</h2>
        
        <div class="content has-text-justified">
          <p>
            <strong>Bridging semantic alignment and physical plausibility.</strong> 
            Existing editing models achieve high semantic fidelity yet frequently violate physical principles, as they learn discrete image mappings with underspecified constraints. 
            We reformulate editing as a <b>Physical State Transition</b>, leveraging continuous dynamics to steer generation from 
            <span style="color:red;">unreal hallucinations</span> toward 
            <span style="color:green;">physically valid trajectories</span>.
          </p>
        </div>

        <img src="static/images/teasor.jpg" alt="Motivation Teaser" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Motivation -->

<!-- Data -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">PhysicTran38K: Video-based dataset of physical state transition</h2>
        <h2 class="content has-text-justified">
          <strong>Overview of the PhysicTran38K construction pipeline.</strong> Starting from hierarchical physics categories, we synthesize videos using Wan2.2-T2V-A14B, filtered by ViPE with an adaptive strategy to preserve high-dynamic transitions. Candidate videos conduct principle-driven verification by GPT-5-mini, adhering to a rigorous retention rule. Finally, Qwen2.5-VL-7B performs constraint-aware annotation, generating instructions and structured reasoning while incorporating verification results to prevent hallucinations.
        </h2>
        <img src="static/images/data.jpg" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Data -->

<!-- Paper Method -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Method Overview</h2>
        <h2 class="content has-text-justified">
          <strong>Overview of the PhysicEdit framework.</strong> (a) Training: We distill physical transition priors from video data into learnable transition queries. These queries are supervised by complementary visual features extracted from intermediate keyframes. (b) Inference: PhysicEdit follows a sequential workflow. The frozen MLLM first generates physically-grounded reasoning, which is then concatenated with the learned transition queries to serve as the condition for the diffusion backbone.
        </h2>
        <img src="static/images/physical_edit.jpg" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End paper method -->



<!-- Qualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Qualitative Results</h2>
        <img src="static/images/small_qualitative.jpg" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Quantitaive -->

<!-- Complex reasoning -->
<!-- <section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Step by step Complex Reasoning Results</h2>
        <img src="static/images/complexreasoning.jpg" width="80%"/>
      </div>
    </div>
  </div>
</section> -->
<!-- End Quantitaive -->
  
<!-- Mainexp -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">GenEval Performance</h2>
        <h2 class="content has-text-justified">
          Quantitative comparisons on PICABench-Superficial and KRIS.
        </h2>
        <img src="static/images/pica.jpg" width="80%"/>
        <img src="static/images/kris.jpg" width="80%"/>
      </div>
    </div>
  </div>
</section>
<!-- End Mainexp -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
